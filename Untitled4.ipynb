{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVyZ_tBMoifm"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import torch\n",
        "from llama_cpp import Llama\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "class RAGSystem:\n",
        "    def __init__(self, model_path, db_path=None):\n",
        "        # Load the quantized model\n",
        "        self.model = Llama(\n",
        "            model_path=model_path,\n",
        "            n_ctx=4096,  # Context window size\n",
        "            n_gpu_layers=-1  # Use GPU if available\n",
        "        )\n",
        "\n",
        "        # Initialize embedding model\n",
        "        self.embedding_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "        # Connect to vector database\n",
        "        self.chroma_client = chromadb.Client()\n",
        "        sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
        "            model_name=\"BAAI/bge-small-en-v1.5\"\n",
        "        )\n",
        "\n",
        "        # Load or create collection\n",
        "        if os.path.exists('./chroma') and db_path:\n",
        "            self.chroma_client = chromadb.PersistentClient(path=db_path)\n",
        "            self.collection = self.chroma_client.get_collection(\n",
        "                name=\"ai_research\",\n",
        "                embedding_function=sentence_transformer_ef\n",
        "            )\n",
        "        else:\n",
        "            self.collection = self.chroma_client.create_collection(\n",
        "                name=\"ai_research\",\n",
        "                embedding_function=sentence_transformer_ef\n",
        "            )\n",
        "            print(\"Warning: No existing vector database found.\")\n",
        "\n",
        "    def retrieve(self, query, n_results=3):\n",
        "        \"\"\"Retrieve relevant chunks for a query\"\"\"\n",
        "        results = self.collection.query(\n",
        "            query_texts=[query],\n",
        "            n_results=n_results\n",
        "        )\n",
        "\n",
        "        documents = results[\"documents\"][0]\n",
        "        metadatas = results[\"metadatas\"][0]\n",
        "\n",
        "        retrieved_text = \"\"\n",
        "        for doc, meta in zip(documents, metadatas):\n",
        "            retrieved_text += f\"\\nFrom {meta['title']} ({meta['source']}):\\n{doc}\\n\"\n",
        "\n",
        "        return retrieved_text\n",
        "\n",
        "    def generate_answer(self, query, context):\n",
        "        \"\"\"Generate an answer using the LLM with context\"\"\"\n",
        "        prompt = f\"\"\"<|im_start|>user\n",
        "I need information about the following topic:\n",
        "{query}\n",
        "\n",
        "Based on the following research paper excerpts:\n",
        "{context}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "        response = self.model(\n",
        "            prompt,\n",
        "            max_tokens=1024,\n",
        "            temperature=0.1,\n",
        "            top_p=0.9,\n",
        "            stop=[\"<|im_end|>\"]\n",
        "        )\n",
        "\n",
        "        return response[\"choices\"][0][\"text\"]\n",
        "\n",
        "    def answer_question(self, query):\n",
        "        \"\"\"End-to-end question answering with RAG\"\"\"\n",
        "        # Retrieve relevant context\n",
        "        context = self.retrieve(query)\n",
        "\n",
        "        # Generate answer\n",
        "        answer = self.generate_answer(query, context)\n",
        "\n",
        "        return answer\n",
        "\n",
        "def main():\n",
        "    # Initialize RAG system\n",
        "    model_path = \"https://drive.google.com/drive/folders/103vmZo3QW9L1RIN3RKvpaABOe9GSX2qR?usp=sharing\"\n",
        "    db_path = \"./chroma\" #HAVE TO INTIALIZE THIS MANUALLY\n",
        "    rag_system = RAGSystem(model_path, db_path)\n",
        "\n",
        "    # Get user input\n",
        "    if len(sys.argv) > 1:\n",
        "        question = sys.argv[1]\n",
        "    else:\n",
        "        question = input(\"Enter your question about AI research: \")\n",
        "\n",
        "    # Generate and print answer\n",
        "    answer = rag_system.answer_question(question)\n",
        "    print(answer)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}